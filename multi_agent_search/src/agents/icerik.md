ğŸ”§ Chunk Ä°yileÅŸtirmeleri - Ã–ncelikli Aksiyonlar
ğŸ”´ KRÄ°TÄ°K: GÃ¶rsel Extraction HatasÄ± (EN Ã–NCELÄ°KLÄ°)
Sorun
Word dÃ¶kÃ¼manÄ±ndan gÃ¶rseller extract edilmemiÅŸ:
âŒ Image not found: /Feb6-Earthquake-Sentiment-Analysis-Research-Paper_images/...
KÃ¶k Neden
load_docx() fonksiyonunda image extraction mantÄ±ÄŸÄ± Ã§alÄ±ÅŸmÄ±yor olabilir:

Word'de gÃ¶rseller media/ klasÃ¶rÃ¼nde (Ã¶rn: media/image1.png)
Ama kodda /Feb6-Earthquake-Sentiment-Analysis-Research-Paper_images/ klasÃ¶rÃ¼ aranÄ±yor
Path uyumsuzluÄŸu â†’ gÃ¶rseller bulunamÄ±yor

Ã‡Ã¶zÃ¼m: load_docx() Debug & Fix
Dosya: src/agents/rag_agent.py â†’ load_docx()
1. Ã–nce Debug: GÃ¶rsel extraction Ã§alÄ±ÅŸÄ±yor mu?
python@tool
def load_docx(file_path: str) -> str:
    from docx import Document as DocxDocument
    from pathlib import Path
    
    logger.info(f"[DOCX DEBUG] Processing {Path(file_path).name}...")
    
    doc = DocxDocument(file_path)
    images_folder = Path(file_path).parent / f"{Path(file_path).stem}_images"
    images_folder.mkdir(exist_ok=True)
    
    # âœ… DEBUG: KaÃ§ gÃ¶rsel var?
    image_count = 0
    for rel_id, rel in doc.part.rels.items():
        if "image" in rel.target_ref:
            image_count += 1
            logger.info(f"[DOCX DEBUG] Found image: {rel.target_ref}")
    
    logger.info(f"[DOCX DEBUG] Total images in document: {image_count}")
    
    if image_count == 0:
        logger.warning("[DOCX DEBUG] âš ï¸ NO IMAGES FOUND - Check if Word file has embedded images")
    
    # ... rest of code
Ã‡alÄ±ÅŸtÄ±r ve loglara bak:

Total images in document: 0 â†’ Word'de gÃ¶rsel yok (beklenmez)
Total images in document: 8 â†’ GÃ¶rseller var AMA extract edilmiyor


2. GÃ¶rsel Path Standardizasyonu
Sorun: Word'den extract edilen gÃ¶rseller farklÄ± path'le kaydediliyor
python# MEVCUT KOD (load_docx iÃ§inde):
image_filename = f"{Path(file_path).stem}-{image_counter}.{ext}"
image_path = images_folder / image_filename

# Store RELATIVE path
relative_path = f"{images_folder.name}/{image_filename}"
image_map[rel_id] = relative_path

# âœ… SORUN: Markdown'a yazÄ±lan path
markdown_parts.append(f"![Image]({relative_path})")
Bu Ã¼retir: ![Image](Feb6-Earthquake-Sentiment-Analysis-Research-Paper_images/Feb6-Earthquake-Sentiment-Analysis-Research-Paper-0.png)
AMA dÃ¶kÃ¼man diyor ki: ![Image](/Feb6-Earthquake-Sentiment-Analysis-Research-Paper_images/Feb6-Earthquake-Sentiment-Analysis-Research-Paper-5.png) (baÅŸÄ±nda / var)
FIX:
python# âœ… Path'i normalize et (baÅŸta / olmasÄ±n)
relative_path = f"{images_folder.name}/{image_filename}"
markdown_parts.append(f"![Image]({relative_path})")  # âœ… DoÄŸru

# âŒ YANLIÅ (baÅŸta / varsa)
markdown_parts.append(f"![Image](/{relative_path})")  # Bu dÃ¶kÃ¼manÄ±ndaki hata

3. GÃ¶rsel Metadata'sÄ±nÄ± Ekle
python# load_docx() sonunda:
logger.info(f"[DOCX] âœ… Converted {len(md_text)} chars with {image_counter} images")
logger.info(f"[DOCX] ğŸ–¼ï¸ Images saved to: {images_folder}")

# âœ… Debug dosyasÄ±na yaz
debug_path = Path(file_path).parent / f"{Path(file_path).stem}_docx_debug.md"
debug_path.write_text(md_text, encoding="utf-8")

# âœ… Image list'i de yaz
image_list_path = Path(file_path).parent / f"{Path(file_path).stem}_image_list.txt"
image_list = [
    f"{idx}: {img_map[rel_id]}"
    for idx, (rel_id, img_map) in enumerate(image_map.items())
]
image_list_path.write_text("\n".join(image_list), encoding="utf-8")
logger.info(f"[DOCX] ğŸ“ Image list saved to: {image_list_path}")
Sonra kontrol et:

uploads/Feb6-Earthquake-Sentiment-Analysis-Research-Paper_docx_debug.md â†’ GÃ¶rsel referanslarÄ± doÄŸru mu?
uploads/Feb6-Earthquake-Sentiment-Analysis-Research-Paper_image_list.txt â†’ Hangi gÃ¶rseller kaydedilmiÅŸ?
uploads/Feb6-Earthquake-Sentiment-Analysis-Research-Paper_images/ â†’ KlasÃ¶rde dosyalar var mÄ±?


ğŸŸ¡ Ã–NEMLI: Metadata ZenginleÅŸtirme
Sorun
Chunk metadata'sÄ± Ã§ok basit:
python{
    "source": "Feb6-Earthquake-Sentiment-Analysis-Research-Paper.docx",
    "has_images": False  # âŒ YanlÄ±ÅŸ
}
Ã‡Ã¶zÃ¼m: Hierarchical Metadata
Dosya: src/agents/agentic_chunker.py â†’ extract_propositions_from_markdown()
1. BÃ¶lÃ¼m BaÅŸlÄ±klarÄ±nÄ± Takip Et
pythondef extract_propositions_from_markdown(markdown_text: str) -> List[str]:
    """Extract propositions with section tracking"""
    if not markdown_text:
        return []
    
    text = markdown_text.replace("\r\n", "\n").replace("\r", "\n")
    
    # âœ… YENÄ°: Track current section (H1/H2)
    current_h1 = None
    current_h2 = None
    
    # Regex patterns
    h1_pattern = re.compile(r'^#\s+(.+)$', re.MULTILINE)
    h2_pattern = re.compile(r'^##\s+(.+)$', re.MULTILINE)
    major_header_pattern = re.compile(r'^(#{1,4})\s+(.+)$', re.MULTILINE)
    
    # Extract all H1/H2 headers for context
    h1_headers = {m.start(): m.group(1) for m in h1_pattern.finditer(text)}
    h2_headers = {m.start(): m.group(1) for m in h2_pattern.finditer(text)}
    
    # ... rest of existing code
    
    # When creating propositions, add section metadata:
    propositions_with_metadata = []
    for prop in propositions:
        # Find which H1/H2 this belongs to
        prop_start = text.find(prop)
        
        # Find nearest H1 before this position
        section_h1 = None
        for pos, title in sorted(h1_headers.items(), reverse=True):
            if pos < prop_start:
                section_h1 = title
                break
        
        # Find nearest H2 before this position
        section_h2 = None
        for pos, title in sorted(h2_headers.items(), reverse=True):
            if pos < prop_start:
                section_h2 = title
                break
        
        # Store as tuple: (proposition, metadata)
        propositions_with_metadata.append({
            'text': prop,
            'section_h1': section_h1,
            'section_h2': section_h2
        })
    
    return propositions_with_metadata

2. AgenticChunker'a Metadata Ekle
Dosya: src/agents/agentic_chunker.py â†’ agentic_chunk_text()
pythondef agentic_chunk_text(text: str, source_name: str) -> List[Document]:
    """Main entry point with rich metadata"""
    
    # Extract propositions WITH metadata
    propositions_with_meta = extract_propositions_from_markdown(text)
    
    # Use agentic chunker
    chunker = AgenticChunker(source_name=source_name)
    
    # âœ… YENÄ°: Pass metadata to chunker
    for prop_data in propositions_with_meta:
        chunker.add_proposition(
            prop_data['text'],
            section_h1=prop_data['section_h1'],
            section_h2=prop_data['section_h2']
        )
    
    documents = chunker.get_documents()
    
    # âœ… YENÄ°: Enrich document metadata
    for doc in documents:
        # Add hierarchical context
        doc.metadata['section_h1'] = doc.metadata.get('section_h1', 'Unknown')
        doc.metadata['section_h2'] = doc.metadata.get('section_h2', None)
        
        # Calculate approximate position (0.0-1.0)
        doc.metadata['position'] = doc.metadata['chunk_index'] / len(documents)
    
    return documents
SonuÃ§:
python# Ã–NCESÄ°
{
    "source": "paper.docx",
    "has_images": False
}

# SONRASI
{
    "source": "paper.docx",
    "section_h1": "YÃ¶ntem",
    "section_h2": "Makine Ã–ÄŸrenmesi",
    "chunk_index": 15,
    "position": 0.45,  # DÃ¶kÃ¼manÄ±n %45'inde
    "has_images": True,
    "title": "Naive Bayes AlgoritmasÄ±",
    "summary": "Bayes teoremine dayanan sÄ±nÄ±flandÄ±rma yÃ¶ntemi"
}

ğŸŸ¢ BONUS: Tablo ve FormÃ¼l Ä°yileÅŸtirme (Opsiyonel)
Sorun 1: Tablo BaÅŸlÄ±klarÄ± Eksik
Chunk #10:
| Kategoriler | Tarih | Åubat | Mart | ...
Ama tablo baÅŸlÄ±ÄŸÄ± ("Tablo 5. Kategori BazÄ±nda...") chunk'ta deÄŸil.
Ã‡Ã¶zÃ¼m: Tablo tespit algoritmasÄ±
pythondef _enhance_table_context(markdown_text: str) -> str:
    """Add table titles if missing"""
    lines = markdown_text.split("\n")
    enhanced = []
    
    i = 0
    while i < len(lines):
        line = lines[i]
        
        # Tablo baÅŸlangÄ±cÄ± mÄ±?
        if line.startswith("|") and i > 0:
            # Ã–nceki 3 satÄ±ra bak - "Tablo X" var mÄ±?
            has_title = False
            for j in range(max(0, i-3), i):
                if "Tablo" in lines[j] or "Table" in lines[j]:
                    has_title = True
                    break
            
            # BaÅŸlÄ±k yoksa ekle
            if not has_title:
                enhanced.append(f"\n**Tablo** (BaÅŸlÄ±k tespit edilemedi)\n")
        
        enhanced.append(line)
        i += 1
    
    return "\n".join(enhanced)

Sorun 2: FormÃ¼ller KayÄ±p
Word'de formÃ¼ller genelde MathML veya gÃ¶rsel olarak gelir. python-docx bunlarÄ± okuyamaz.
Alternatif:

KullanÄ±cÄ±ya uyar: "Bu dÃ¶kÃ¼manda formÃ¼ller olabilir, PDF versiyonunu yÃ¼kleyin"
Placeholder ekle: FormÃ¼l yerine [Formula: Kesinlik = TP/(TP+FP)]

python# load_docx() iÃ§inde:
# FormÃ¼l tespit (heuristic - "=" var ama tablo deÄŸil)
if "=" in para.text and "|" not in para.text:
    markdown_parts.append(f"[Formula: {para.text.strip()}]")

ğŸ“‹ Uygulama SÄ±rasÄ±
AdÄ±m 1: GÃ¶rsel Debug (5 dakika)
bash# Word dosyasÄ±nÄ± yeniden yÃ¼kle
# Loglara bak:
# - Total images in document: ?
# - Images saved to: ?
AdÄ±m 2: GÃ¶rsel Path Fix (10 dakika)

load_docx() iÃ§inde relative path'i dÃ¼zelt
_docx_debug.md kontrol et
Vision model test et

AdÄ±m 3: Metadata ZenginleÅŸtirme (20 dakika)

extract_propositions_from_markdown() â†’ section tracking ekle
agentic_chunk_text() â†’ metadata propagation
Test query: "YÃ¶ntem bÃ¶lÃ¼mÃ¼nde hangi algoritmalar kullanÄ±lmÄ±ÅŸ?"

AdÄ±m 4: Tablo Ä°yileÅŸtirme (Opsiyonel, 10 dakika)

_enhance_table_context() fonksiyonu ekle
Tablo baÅŸlÄ±klarÄ±nÄ± otomatik ekle


ğŸ¯ Beklenen Ä°yileÅŸme
Ã–ncesi
Chunk #11:
"Naive Bayes algoritmasÄ±nÄ±n temel Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirmektedir..."

Metadata: {
    "source": "paper.docx",
    "has_images": False  # âŒ YanlÄ±ÅŸ
}

âŒ Image not found: /paper_images/image2.png
SonrasÄ±
Chunk #11:
"##### Naive Bayes

Naive Bayes algoritmasÄ±nÄ±n temel Ã§alÄ±ÅŸma mantÄ±ÄŸÄ±nÄ± gÃ¶rselleÅŸtirmektedir...

![Naive Bayes DiyagramÄ±](paper_images/paper-2.png)"

Metadata: {
    "source": "paper.docx",
    "section_h1": "YÃ¶ntem",
    "section_h2": "Makine Ã–ÄŸrenmesi",
    "title": "Naive Bayes AlgoritmasÄ±",
    "has_images": True,
    "position": 0.42,
    "chunk_index": 11
}

âœ… Vision Model: "GÃ¶rsel bir funnel (huni) diyagramÄ± gÃ¶stermektedir..."

ğŸ’¡ HÄ±zlÄ± Test
python# Test script: test_chunks.py
from src.agents.rag_agent import load_docx, ingest_text

# 1. Word'Ã¼ yÃ¼kle
md_text = load_docx("uploads/paper.docx")

# 2. Markdown kontrol et
with open("test_output.md", "w", encoding="utf-8") as f:
    f.write(md_text)

print("âœ… Markdown kaydedildi: test_output.md")
print(f"- Uzunluk: {len(md_text)} chars")
print(f"- GÃ¶rsel sayÄ±sÄ±: {md_text.count('![')}")

# 3. Chunk'la
chunk_count = ingest_text(md_text, "test_paper.docx")
print(f"âœ… {chunk_count} chunk oluÅŸturuldu")

# 4. Query test
from src.agents.rag_agent import retrieve_context
context, docs = retrieve_context("Naive Bayes nasÄ±l Ã§alÄ±ÅŸÄ±r?", top_k="3")

print(f"\nğŸ“Š Retrieval SonuÃ§larÄ±:")
print(f"- Bulunan dÃ¶kÃ¼man: {len(docs)}")
print(f"- Ä°lk chunk metadata: {docs[0].metadata if docs else 'N/A'}")