"""
Multi-Agent Research System - Simplified Version
Direct LLM calls without complex tool calling (Ollama compatible)
"""

import os
import asyncio
import httpx
import json
import uuid
from typing import Optional, AsyncGenerator, Any
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_mcp_adapters.client import MultiServerMCPClient
from src.config import settings
from src.models import get_llm_model, sanitize_tool_schema
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============ LANGSMITH ============
def setup_langsmith():
    """LangSmith tracing'i multi-agent iÃ§in aktifleÅŸtir"""
    if settings.langsmith_api_key:
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = "ai-research-multi-agent-v2"
        os.environ["LANGCHAIN_API_KEY"] = settings.langsmith_api_key
        os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
        logger.info("[LANGSMITH] Multi-Agent v2 aktif - https://smith.langchain.com/o/personal/projects/p/ai-research-multi-agent-v2")
        return True
    return False


# =============================================================================
# FIRECRAWL DIRECT API (MCP yerine doÄŸrudan API Ã§aÄŸrÄ±sÄ±)
# =============================================================================

async def firecrawl_search(query: str, limit: int = 5) -> dict:
    """Firecrawl API ile doÄŸrudan web aramasÄ±.

    Returns a dict with:
      - provider: str
      - text: str (LLM-facing)
      - sources: list[{title,url}]
    """
    api_key = settings.firecrawl_api_key
    if not api_key:
        return {"provider": "firecrawl", "text": "Firecrawl API key bulunamadÄ±", "sources": []}
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "https://api.firecrawl.dev/v1/search",
                headers={
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "query": query,
                    "limit": limit,
                    "scrapeOptions": {
                        "formats": ["markdown"],
                        "onlyMainContent": True
                    }
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                results = data.get("data", [])
                if not results:
                    return {"provider": "firecrawl", "text": f"'{query}' iÃ§in sonuÃ§ bulunamadÄ±", "sources": []}
                
                # SonuÃ§larÄ± formatla
                output = []
                sources = []
                for i, item in enumerate(results[:limit], 1):
                    title = item.get("title", "BaÅŸlÄ±k yok")
                    url = item.get("url", "")
                    content = item.get("markdown", item.get("description", ""))[:1500]
                    if url:
                        sources.append({"title": title, "url": url})
                    output.append(f"### {i}. {title}\nURL: {url}\n\n{content}\n")

                return {"provider": "firecrawl", "text": "\n---\n".join(output), "sources": sources}
            else:
                logger.warning(f"Firecrawl API hatasÄ±: {response.status_code}")
                return {"provider": "firecrawl", "text": f"Firecrawl API hatasÄ±: {response.status_code}", "sources": []}
    except Exception as e:
        logger.error(f"Firecrawl hatasÄ±: {e}")
        return {"provider": "firecrawl", "text": f"Arama hatasÄ±: {str(e)}", "sources": []}


async def tavily_search(query: str, limit: int = 5) -> dict:
    """Tavily API ile web aramasÄ± (yedek).

    Returns a dict with:
      - provider: str
      - text: str (LLM-facing)
      - sources: list[{title,url}]
    """
    api_key = settings.tavily_api_key if hasattr(settings, 'tavily_api_key') else os.getenv("TAVILY_API_KEY")
    if not api_key:
        return {"provider": "tavily", "text": "", "sources": []}
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": api_key,
                    "query": query,
                    "max_results": limit,
                    "include_answer": True
                }
            )
            
            if response.status_code == 200:
                data = response.json()
                results = data.get("results", [])
                answer = data.get("answer", "")
                
                output = []
                sources = []
                if answer:
                    output.append(f"**Ã–zet:** {answer}\n")
                
                for i, item in enumerate(results[:limit], 1):
                    title = item.get("title", "")
                    url = item.get("url", "")
                    content = item.get("content", "")[:800]
                    if url:
                        sources.append({"title": title, "url": url})
                    output.append(f"### {i}. {title}\nURL: {url}\n\n{content}\n")

                return {"provider": "tavily", "text": "\n---\n".join(output), "sources": sources}
    except Exception as e:
        logger.warning(f"Tavily hatasÄ±: {e}")
    
    return {"provider": "tavily", "text": "", "sources": []}


# =============================================================================
# HYBRID LLM WRAPPER
# - Groq: Router / tool selection / final synthesis
# - Ollama: Small steps (research draft, code draft)
# =============================================================================

_groq_model = None
_local_model = None


def _get_groq_model():
    """Groq model lazy loading (router + final synthesis)."""
    global _groq_model
    if _groq_model is not None:
        return _groq_model

    provider, model_name = settings.get_model_provider(settings.default_model)
    if provider != "groq":
        # Misconfig fallback
        _groq_model = get_llm_model()
        return _groq_model

    api_key = getattr(settings, "groq_api_key", None)
    if not api_key:
        # Misconfig fallback
        _groq_model = get_llm_model()
        return _groq_model

    from langchain_groq import ChatGroq
    _groq_model = ChatGroq(model=model_name, api_key=api_key, temperature=0.2)
    return _groq_model


def _get_local_model():
    """Local Ollama model lazy loading (cheap small steps)."""
    global _local_model
    if _local_model is not None:
        return _local_model

    # Prefer SECONDARY_MODEL if it's ollama:...
    provider, model_name = settings.get_model_provider(settings.secondary_model)
    if provider != "ollama":
        model_name = os.getenv("LOCAL_MODEL", "llama3.1:8b")

    from langchain_ollama import ChatOllama
    _local_model = ChatOllama(
        model=model_name,
        base_url=settings.ollama_base_url,
        temperature=0.3,
    )
    return _local_model


def _is_retryable_llm_error(err: Exception) -> bool:
    msg = str(err).lower()
    return (
        "429" in msg
        or "rate" in msg
        or "quota" in msg
        or "resource_exhausted" in msg
        or "temporarily" in msg
        or "timeout" in msg
    )


async def call_llm(
    system_prompt: str,
    user_prompt: str,
    role: str = "agent",
    retries: int = 2
) -> str:
    """
    LLM call with hybrid model routing (Groq for critical, Ollama for heavy lifting).
    
    Args:
        system_prompt: System message
        user_prompt: User message
        role: 'synthesis' for final (Groq), 'agent' for heavy work (Ollama)
        retries: Retry count
    """
    # Hybrid routing: Groq for final synthesis, Ollama for token-heavy agent work
    if role == "synthesis":
        try:
            model = _get_groq_model()
            logger.info("[HYBRID] Using Groq for final synthesis")
        except Exception as e:
            logger.warning(f"[HYBRID] Groq unavailable, falling back to local: {e}")
            model = _get_local_model()
    else:
        model = _get_local_model()
        logger.info(f"[HYBRID] Using Ollama for {role} (token-heavy work)")
    
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ]
    
    # Generate proper UUID for LangSmith tracing
    run_id = str(uuid.uuid4())
    config = {"run_id": run_id, "run_name": f"multi-agent-{role}"}
    
    last_err: Exception | None = None
    for attempt in range(retries + 1):
        try:
            response = await model.ainvoke(messages, config=config)
            return response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            last_err = e
            if attempt < retries and _is_retryable_llm_error(e):
                delay = 1.0 * (2 ** attempt)
                logger.warning(f"[WARN] LLM retryable error, retrying in {delay:.1f}s: {e}")
                await asyncio.sleep(delay)
                continue
            break

    logger.error(f"LLM hatasÄ±: {last_err}")
    return f"Hata: {str(last_err)}"


# =============================================================================
# AGENT PROMPTS (Simple, direct prompts for Ollama)
# =============================================================================

RESEARCHER_PROMPT = """Sen uzman bir web araÅŸtÄ±rmacÄ±sÄ±sÄ±n. 

AÅŸaÄŸÄ±daki web arama sonuÃ§larÄ±nÄ± kullanarak kullanÄ±cÄ±nÄ±n sorusuna kapsamlÄ± bir araÅŸtÄ±rma Ã¶zeti hazÄ±rla.

ðŸ“‹ GÃ¶revlerin:
1. Web sonuÃ§larÄ±nÄ± dikkatlice analiz et
2. En Ã¶nemli bilgileri Ã§Ä±kar ve Ã¶zetle
3. FarklÄ± kaynaklarÄ± karÅŸÄ±laÅŸtÄ±r
4. GÃ¼ncel ve doÄŸrulanmÄ±ÅŸ bilgileri vurgula
5. Kaynak linklerini belirt

âš ï¸ Kurallar:
- EN AZ 500 kelime yaz
- Madde madde ve organize ol
- Kaynak linklerini mutlaka ekle
- Spesifik veriler, tarihler, istatistikler kullan
- "AraÅŸtÄ±rma sonucunda..." diye baÅŸlama, direkt bilgiyi ver"""

CODER_PROMPT = """Sen uzman bir yazÄ±lÄ±m geliÅŸtiricisisin.

AÅŸaÄŸÄ±daki araÅŸtÄ±rma sonuÃ§larÄ±nÄ± baz alarak konuyla ilgili pratik, Ã§alÄ±ÅŸan kod Ã¶rnekleri yaz.

ðŸ“‹ GÃ¶revlerin:
1. Temel kullanÄ±m Ã¶rneÄŸi (yeni baÅŸlayanlar iÃ§in)
2. Orta seviye Ã¶rnek (gerÃ§ek dÃ¼nya senaryosu)
3. Ä°leri seviye Ã¶rnek (best practices)
4. Her kod bloÄŸunu aÃ§Ä±klayÄ±cÄ± yorumlarla destekle

âš ï¸ Kurallar:
- Kod Ã‡ALIÅžMALI (syntax hatasÄ± olmasÄ±n)
- Her Ã¶rneÄŸi kÄ±sa aÃ§Ä±klamayla tanÄ±t
- Import statement'larÄ±nÄ± dahil et
- Modern syntax kullan
- Minimum 3 farklÄ± Ã¶rnek ver"""

WRITER_PROMPT = """Sen uzman bir teknik yazar ve eÄŸitmensin. Verilen araÅŸtÄ±rma ve kod Ã¶rneklerinden yola Ã§Ä±karak profesyonel, kapsamlÄ± ve anlaÅŸÄ±lÄ±r bir TÃ¼rkÃ§e makale oluÅŸturacaksÄ±n.

Markdown formatÄ±nda yaz. ChatGPT tarzÄ±nda temiz, modern ve akÄ±cÄ± bir yapÄ± kullan:

# [Konu BaÅŸlÄ±ÄŸÄ±]

Konuya giriÅŸ paragrafÄ± (2-3 cÃ¼mle) - ne, neden Ã¶nemli?

## Genel BakÄ±ÅŸ

Konunun temellerini aÃ§Ä±kla. Okuyucunun neyi Ã¶ÄŸreneceÄŸini net ÅŸekilde belirt.

## Ana Kavramlar

Her bir Ã¶nemli kavramÄ± ayrÄ± alt baÅŸlÄ±k altÄ±nda detaylÄ± aÃ§Ä±kla:

### [Kavram 1]
AÃ§Ä±klama ve detaylar...

### [Kavram 2]  
AÃ§Ä±klama ve detaylar...

## KarÅŸÄ±laÅŸtÄ±rma (eÄŸer uygunsa)

Alternatifleri veya farklÄ± yaklaÅŸÄ±mlarÄ± karÅŸÄ±laÅŸtÄ±r. Avantaj/dezavantajlarÄ± dengeli ÅŸekilde sun.

## Pratik KullanÄ±m

GerÃ§ek dÃ¼nya senaryolarÄ±nda nasÄ±l kullanÄ±lÄ±r? Somut Ã¶rnekler ver.

## Kod Ã–rnekleri

```kod-dili
// AÃ§Ä±klamalÄ±, anlaÅŸÄ±lÄ±r kod Ã¶rnekleri
// Her Ã¶rneÄŸi kÄ±sa aÃ§Ä±klamayla sun
```

## En Ä°yi Uygulamalar

- Liste formatÄ±nda, pratik Ã¶neriler
- Her madde somut ve uygulanabilir olmalÄ±
- YaygÄ±n hatalardan kaÃ§Ä±nma yollarÄ±

## Kaynaklar

AraÅŸtÄ±rmadan gelen gÃ¼venilir kaynaklarÄ± listele.

---

**Kurallar:**
- AkÄ±cÄ±, doÄŸal TÃ¼rkÃ§e kullan
- Emoji kullanma (token israfÄ±)
- Gereksiz formatlamadan kaÃ§Ä±n
- Her bÃ¶lÃ¼mÃ¼ anlamlÄ± iÃ§erikle doldur
- Minimum 800 kelime hedefle"""


ROUTER_PROMPT = """Sen bir supervisor/router'sÄ±n. AmaÃ§: Groq'u minimum kullanÄ±p iÅŸleri yerelde (Ollama) yaptÄ±rmak.

Sadece aÅŸaÄŸÄ±daki JSON'u dÃ¶ndÃ¼r (baÅŸka hiÃ§bir ÅŸey yazma):

{
    "web_search": "none" | "tavily" | "both",
    "need_code": true | false,
    "need_long_report": true | false
}

Kurallar:
- Soru gÃ¼ncel bilgi/versiyon/istatistik iÃ§eriyorsa web_search='both'
- Basit tanÄ±m/Ã¶zet sorularÄ±nda web_search='tavily'
- Tamamen genel ve kÃ¼Ã§Ã¼k bir iÅŸse web_search='none'
- Kod isteniyorsa need_code=true
"""


# =============================================================================
# MAIN PIPELINE (Simplified - No DeepAgents, Direct LLM calls)
# =============================================================================

async def run_multi_agent_research(
    query: str,
    verbose: bool = True,
    options: Optional[dict] = None,
) -> AsyncGenerator:
    """
    Simplified Multi-Agent Pipeline - Ollama Compatible
    
    1. Web Search (Firecrawl + Tavily)
    2. Researcher LLM (analyze search results)
    3. Coder LLM (generate code examples)
    4. Writer LLM (create final report)
    """
    setup_langsmith()
    
    logger.info(f"[PIPELINE] BaÅŸlatÄ±lÄ±yor: {query[:50]}...")
    
    try:
        # 0. PLANNING
        yield {
            "status": "planning",
            "message": "Plan oluÅŸturuluyor...",
            "agent": "supervisor"
        }

        # Router decision (Groq - synthesis role for critical decisions)
        router_raw = await call_llm(
            ROUTER_PROMPT,
            f"KullanÄ±cÄ± sorusu: {query}\nJSON Ã¼ret.",
            role="synthesis",
            retries=1,
        )

        route = {"web_search": "both", "need_code": True, "need_long_report": True}
        try:
            # some models wrap in ```json blocks
            cleaned = router_raw.strip()
            if cleaned.startswith("```"):
                cleaned = cleaned.strip("`")
                cleaned = cleaned.replace("json", "", 1).strip()
            route = {**route, **json.loads(cleaned)}
        except Exception:
            pass

        # Apply optional user overrides (shared state from UI)
        if isinstance(options, dict):
            if options.get("web_search") in ("none", "tavily", "both"):
                route["web_search"] = options["web_search"]
            if isinstance(options.get("need_code"), bool):
                route["need_code"] = bool(options["need_code"])
            if isinstance(options.get("need_long_report"), bool):
                route["need_long_report"] = bool(options["need_long_report"])

        web_search_mode = route.get("web_search", "both")
        if web_search_mode not in ("none", "tavily", "both"):
            web_search_mode = "both"
        need_code = bool(route.get("need_code", True))
        need_long_report = bool(route.get("need_long_report", True))
        
        # 1. WEB SEARCH (Direct API calls - costs credits; router can reduce)
        yield {
            "status": "searching",
            "message": (
                "Web aramasÄ± atlandÄ± (router kararÄ±)" if web_search_mode == "none" else
                ("Web aramasÄ± yapÄ±lÄ±yor (Tavily)" if web_search_mode == "tavily" else "Web aramasÄ± yapÄ±lÄ±yor (Firecrawl + Tavily)")
            ),
            "agent": "search"
        }
        logger.info("[1/4] Web Search baÅŸlÄ±yor...")

        search_results = ""
        sources: list[dict] = []
        if web_search_mode == "none":
            search_results = "Web aramasÄ± router tarafÄ±ndan atlandÄ±. Genel bilgiyle devam." 
        else:
            tasks = []
            # Tavily (cheaper) always when searching
            tasks.append(tavily_search(query, limit=3))
            # Firecrawl only on 'both'
            if web_search_mode == "both":
                tasks.append(firecrawl_search(query, limit=5))

            results = await asyncio.gather(*tasks, return_exceptions=True)

            tavily_pack = results[0] if results else {"provider": "tavily", "text": "", "sources": []}
            firecrawl_pack = results[1] if len(results) > 1 else {"provider": "firecrawl", "text": "", "sources": []}

            if isinstance(tavily_pack, dict) and tavily_pack.get("text"):
                search_results += f"## Tavily SonuÃ§larÄ±:\n{tavily_pack.get('text','')}\n\n"
                if isinstance(tavily_pack.get("sources"), list):
                    for s in tavily_pack["sources"]:
                        if isinstance(s, dict) and s.get("url"):
                            sources.append({"provider": "tavily", **s})

            if isinstance(firecrawl_pack, dict) and firecrawl_pack.get("text"):
                search_results += f"## Firecrawl SonuÃ§larÄ±:\n{firecrawl_pack.get('text','')}\n\n"
                if isinstance(firecrawl_pack.get("sources"), list):
                    for s in firecrawl_pack["sources"]:
                        if isinstance(s, dict) and s.get("url"):
                            sources.append({"provider": "firecrawl", **s})
        logger.info(f"[1/4] Web Search tamamlandÄ±: {len(search_results)} karakter")

        # Emit structured sources for tool-card rendering in the UI
        if sources:
            yield {
                "status": "searching",
                "message": "Kaynaklar derlendi",
                "agent": "search",
                "meta": {"sources": sources[:20]},
            }
        
        # 2. RESEARCHER - Analyze search results
        yield {
            "status": "researching",
            "message": "AraÅŸtÄ±rma sonuÃ§larÄ± analiz ediliyor",
            "agent": "researcher"
        }
        logger.info("[2/4] Researcher baÅŸlÄ±yor... (LOCAL)")
        
        researcher_input = f"""KullanÄ±cÄ± Sorusu: {query}

Web Arama SonuÃ§larÄ±:
{search_results[:8000]}

YukarÄ±daki kaynaklara dayanarak kapsamlÄ± bir araÅŸtÄ±rma Ã¶zeti hazÄ±rla."""
        
        # Researcher uses Ollama (agent role for token-heavy work)
        research_result = await call_llm(
            RESEARCHER_PROMPT,
            researcher_input,
            role="agent",
            retries=0,
        )
        logger.info(f"[2/4] Researcher tamamlandÄ±: {len(research_result)} karakter (Ollama - token-heavy)")
        
        # 3. CODER - Generate code examples
        yield {
            "status": "coding",
            "message": "Kod Ã¶rnekleri hazÄ±rlanÄ±yor",
            "agent": "coder"
        }
        logger.info("[3/4] Coder baÅŸlÄ±yor... (LOCAL)")
        
        coder_input = f"""Konu: {query}

AraÅŸtÄ±rma Ã–zeti:
{research_result[:4000]}

Bu konuyla ilgili pratik kod Ã¶rnekleri yaz."""
        
        if need_code:
            # Coder uses Ollama (agent role for token-heavy work)
            code_result = await call_llm(
                CODER_PROMPT,
                coder_input,
                role="agent",
                retries=0,
            )
            logger.info(f"[3/4] Coder tamamlandÄ±: {len(code_result)} karakter (Ollama - token-heavy)")
        else:
            code_result = "(Router kararÄ±yla kod Ã¶rnekleri atlandÄ±.)"
            logger.info("[3/4] Coder atlandÄ± (router)")
        
        # 4. WRITER - Create final report
        yield {
            "status": "writing",
            "message": "Final rapor yazÄ±lÄ±yor",
            "agent": "writer"
        }
        logger.info("[4/4] Writer baÅŸlÄ±yor... (GROQ)")
        
        writer_input = f"""Konu: {query}

## AraÅŸtÄ±rma SonuÃ§larÄ±:
{research_result[:5000]}

## Kod Ã–rnekleri:
{code_result[:3000]}

## Web KaynaklarÄ±:
{search_results[:2000]}

YukarÄ±daki tÃ¼m bilgileri kullanarak kapsamlÄ± bir TÃ¼rkÃ§e eÄŸitim makalesi yaz."""
        
        # Writer uses Groq (synthesis role for quality output)
        final_report = await call_llm(
            WRITER_PROMPT,
            writer_input,
            role="synthesis",
            retries=2,
        )
        logger.info(f"[4/4] Writer tamamlandÄ±: {len(final_report)} karakter (Groq - final synthesis)")
        
        # 5. DONE
        logger.info("[OK] Multi-Agent pipeline tamamlandÄ±!")
        yield {
            "status": "done",
            "message": "TamamlandÄ±",
            "content": final_report
        }
    
    except Exception as e:
        error_msg = f"Multi-Agent hatasÄ±: {str(e)}"
        logger.error(f"[ERROR] {error_msg}", exc_info=True)
        yield {
            "status": "error",
            "message": f"Hata: {error_msg}",
            "content": f"# {query}\n\nHata: {error_msg}"
        }


# =============================================================================
# BACKWARD COMPATIBILITY
# =============================================================================

async def run_multi_agent_research_old(query: str, verbose: bool = True) -> str:
    """Eski API - yeni versiyonu Ã§aÄŸÄ±rÄ±r"""
    async for update in run_multi_agent_research(query, verbose):
        if update.get("status") == "done":
            return update.get("content", "")
    return ""

