"""
Basit Tek-Agent Sistem - Rate Limit Dostu
Gemini Free Tier iÃ§in optimize edilmiÅŸ (20 istek/gÃ¼n)
"""

import asyncio
import logging
from typing import Optional
from langchain_mcp_adapters.client import MultiServerMCPClient
from src.config import settings
from src.models import get_llm_model, sanitize_tool_schema
from deepagents import create_deep_agent

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ============================================================================
# CACHE SÄ°STEMÄ° - API Ã‡AÄRILARINI AZALTIR
# ============================================================================

_search_cache = {}

def cache_search_result(query: str, result: str):
    """Arama sonuÃ§larÄ±nÄ± Ã¶nbelleÄŸe al"""
    _search_cache[query.lower().strip()] = result

def get_cached_search(query: str) -> Optional[str]:
    """Ã–nbellekten arama sonucu getir"""
    return _search_cache.get(query.lower().strip())


# ============================================================================
# RATE LIMIT AWARE AGENT
# ============================================================================

SIMPLE_RESEARCH_PROMPT = """Sen TÃ¼rkÃ§e araÅŸtÄ±rma yapan bir AI asistanÄ±sÄ±n.

ğŸ¯ GÃ–REVÄ°N:
1. **TEK BÄ°R** tool kullan (en fazla 1 arama)
2. DetaylÄ± TÃ¼rkÃ§e rapor yaz
3. HEMEN DURDUR

ğŸ› ï¸ ARAÃ‡LAR:
- firecrawl_search(query) - Web scraping
- tavily-search(query, max_results=3) - AI search (Ã–NER: Daha hÄ±zlÄ±)

ğŸ“‹ RAPOR FORMATI:
# ğŸ“Š [BaÅŸlÄ±k]

## ğŸ¯ Ã–zet
[2-3 cÃ¼mle Ã¶zet]

## ğŸ“– DetaylÄ± AÃ§Ä±klama
[En az 3 paragraf - Nedir? NasÄ±l Ã§alÄ±ÅŸÄ±r? Neden Ã¶nemli?]

## ğŸ’» Kod Ã–rnekleri (EÄŸer teknik konuysa)
```python
# Ã–rnek 1: Basit kullanÄ±m
kod_burada()
```
**AÃ§Ä±klama:** Ne yaptÄ±ÄŸÄ±

```python
# Ã–rnek 2: GeliÅŸmiÅŸ
advanced_kod()
```

## ğŸ¯ KullanÄ±m AlanlarÄ±
- Alan 1
- Alan 2
- Alan 3

## âœ… Avantajlar & âŒ Dezavantajlar

### âœ… ArtÄ±larÄ±:
- ArtÄ± 1
- ArtÄ± 2

### âŒ Eksileri:
- Eksi 1

## ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§
1. AdÄ±m 1
2. AdÄ±m 2
3. AdÄ±m 3

## ğŸ“š Kaynaklar
- [Kaynak 1](url)
- [Kaynak 2](url)

---
**ğŸ” Kaynak:** [Tool adÄ±]

âš ï¸ KURALLAR:
- SADECE 1 TOOL KULLAN (fazlasÄ± rate limit!)
- En az 3 paragraf yaz
- Teknik konularda mutlaka kod Ã¶rnekleri ver
- TÃ¼rkÃ§e yaz (kod hariÃ§)
- AraÅŸtÄ±rma sonrasÄ± HEMEN DURDUR
"""


async def create_simple_agent():
    """Rate-limit dostu basit agent"""
    
    logger.info("ğŸ”Œ MCP baÄŸlanÄ±yor...")
    
    # Sadece Tavily kullan (daha az API Ã§aÄŸrÄ±sÄ±)
    servers = {}
    
    # Tavily varsa onu kullan (Firecrawl'dan daha ekonomik)
    if hasattr(settings, 'tavily_api_key') and settings.tavily_api_key:
        servers["tavily"] = {
            "command": "npx",
            "args": ["-y", "tavily-mcp@latest"],
            "env": {"TAVILY_API_KEY": settings.tavily_api_key},
            "transport": "stdio"
        }
        logger.info("   âœ… Tavily MCP aktif (Ã¶nerilen)")
    
    # Yoksa Firecrawl
    if not servers and settings.firecrawl_api_key:
        servers["firecrawl"] = {
            "command": settings.firecrawl_mcp_command,
            "args": settings.firecrawl_mcp_args,
            "env": settings.get_firecrawl_env(),
            "transport": "stdio"
        }
        logger.info("   âœ… Firecrawl MCP aktif")
    
    if not servers:
        raise ValueError("âŒ En az bir MCP server gerekli (Tavily veya Firecrawl)")
    
    mcp_client = MultiServerMCPClient(servers)
    tools = await mcp_client.get_tools()
    
    for tool in tools:
        sanitize_tool_schema(tool)
    
    logger.info(f"   ğŸ“‹ {len(tools)} tool yÃ¼klendi")
    
    # Model
    model = get_llm_model()
    
    # Agent - DÃœÅÃœK RECURSION!
    agent = create_deep_agent(
        model=model,
        instructions=SIMPLE_RESEARCH_PROMPT,
        tools=tools
    )
    
    logger.info("âœ… Simple agent hazÄ±r\n")
    return agent, mcp_client


async def run_simple_research(query: str, verbose: bool = True) -> str:
    """Basit araÅŸtÄ±rma - Rate limit dostu"""
    
    # Cache kontrolÃ¼
    cached = get_cached_search(query)
    if cached:
        logger.info("ğŸ“¦ Ã–nbellekten sonuÃ§ dÃ¶ndÃ¼rÃ¼lÃ¼yor")
        if verbose:
            print("\nğŸ’¾ (Ã–nbellekten)\n")
        return cached
    
    agent = None
    mcp_client = None
    
    try:
        if verbose:
            print("\n" + "="*70)
            print("ğŸ”¬ Basit AraÅŸtÄ±rma (Rate-Limit Dostu)")
            print("="*70)
            print(f"ğŸ“ Soru: {query}\n")
        
        agent, mcp_client = await create_simple_agent()
        
        logger.info("ğŸš€ AraÅŸtÄ±rma baÅŸlatÄ±lÄ±yor...")
        
        # DÃœÅÃœK RECURSION LIMIT!
        result = await agent.ainvoke(
            {"messages": [{"role": "user", "content": query}]},
            config={"recursion_limit": 5}  # AZALTILDI: 15 â†’ 5
        )
        
        # Son mesajÄ± bul
        final_response = ""
        if "messages" in result:
            for msg in reversed(result["messages"]):
                if hasattr(msg, 'content') and isinstance(msg.content, str) and msg.content.strip():
                    final_response = msg.content.strip()
                    break
        
        if not final_response:
            final_response = "âŒ YanÄ±t Ã¼retilemedi"
        
        # Cache'e kaydet
        cache_search_result(query, final_response)
        
        if verbose:
            print("\n" + "="*70)
            print("ğŸ“Š SONUÃ‡")
            print("="*70)
            print(final_response)
            print("="*70)
        
        return final_response
        
    except Exception as e:
        error_msg = str(e)
        
        if "429" in error_msg or "quota" in error_msg.lower():
            logger.error("âŒ Rate limit aÅŸÄ±ldÄ±! Ã‡Ã¶zÃ¼mler:")
            print("\nâš ï¸ GEMÄ°NÄ° API LÄ°MÄ°TÄ° AÅILDI (429)")
            print("\nğŸ”§ Ã‡Ã–ZÃœMLER:")
            print("1. â° 24 saat bekleyin (gÃ¼nlÃ¼k 20 istek)")
            print("2. ğŸ’³ Gemini API'yi Ã¼cretli yapÄ±n")
            print("3. ğŸ”„ FarklÄ± bir Google hesabÄ± kullanÄ±n")
            print("4. ğŸ  Ollama ile local model Ã§alÄ±ÅŸtÄ±rÄ±n:")
            print("   â€¢ ollama pull llama3.2")
            print("   â€¢ .env â†’ DEFAULT_MODEL=ollama:llama3.2")
            return "âŒ Rate limit aÅŸÄ±ldÄ±. YukarÄ±daki Ã§Ã¶zÃ¼mlere bakÄ±n."
        
        logger.error(f"âŒ Hata: {error_msg}")
        return f"âŒ Hata: {error_msg}"
        
    finally:
        if mcp_client:
            try:
                await mcp_client.close()
            except:
                pass


# ============================================================================
# BATCH ARAÅTIRMA - GÃœNLÃœK LÄ°MÄ°TÄ° PLANLAYARAK KULLAN
# ============================================================================

async def run_batch_research(queries: list[str], delay: int = 5) -> dict[str, str]:
    """
    Birden fazla soruyu sÄ±rasÄ± ile araÅŸtÄ±rÄ±r
    
    Args:
        queries: Sorular listesi
        delay: Her soru arasÄ±nda bekleme (saniye)
    
    Returns:
        {soru: yanÄ±t} dictionary
    """
    
    results = {}
    
    print(f"\nğŸ“¦ Batch araÅŸtÄ±rma: {len(queries)} soru")
    print(f"   â±ï¸ Her soru arasÄ± {delay}s bekleme")
    print(f"   â° Tahmini sÃ¼re: {len(queries) * delay // 60} dakika\n")
    
    for i, query in enumerate(queries, 1):
        print(f"\n[{i}/{len(queries)}] {query}")
        
        try:
            result = await run_simple_research(query, verbose=False)
            results[query] = result
            print("   âœ… TamamlandÄ±")
            
            # Rate limit iÃ§in bekle
            if i < len(queries):
                print(f"   â³ {delay}s bekleniyor...")
                await asyncio.sleep(delay)
                
        except Exception as e:
            results[query] = f"âŒ Hata: {str(e)}"
            print(f"   âŒ BaÅŸarÄ±sÄ±z: {str(e)}")
    
    return results
